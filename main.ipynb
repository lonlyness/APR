{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load the trainMatrix...\n",
      "already load the trainList...\n"
     ]
    }
   ],
   "source": [
    "from Dataset import Dataset\n",
    "dataset = Dataset(\"Data/yelp\")\n",
    "\n",
    "# \"\"\"\n",
    "#         self.trainMatrix = self.load_training_file_as_matrix(path + \".train.rating\")\n",
    "#         self.trainList = self.load_training_file_as_list(path + \".train.rating\")\n",
    "#         self.testRatings = self.load_rating_file_as_list(path + \".test.rating\") #\n",
    "#         self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        \n",
    "#         trainMatrix：クラスDataの疎行列としての負荷評価レコード\n",
    "#         trianList：評価レコードをリストとしてロードして、ユーザーの機能検索を高速化します。\n",
    "#         testRatings：クラスEvaluateのリーブアウトワンアウト評価テスト\n",
    "#         testNegatives：ユーザーによって評価されていない項目をサンプリングします\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(dataset):\n",
    "    _user_input, _item_input_pos = [], []\n",
    "    for (u, i) in dataset.trainMatrix.keys():\n",
    "        # positive instance\n",
    "        _user_input.append(u)\n",
    "        _item_input_pos.append(i)\n",
    "    return _user_input, _item_input_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampling(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def shuffle(samples, batch_size, dataset):\n",
    "    global _user_input\n",
    "    global _item_input_pos\n",
    "    global _batch_size\n",
    "    global _index\n",
    "    \n",
    "    global _dataset\n",
    "    _user_input, _item_input_pos = samples\n",
    "    _batch_size = batch_size\n",
    "    _dataset = dataset\n",
    "    _index = range(len(_user_input))\n",
    "    np.random.shuffle(_index)\n",
    "    num_batch = len(_user_input) // _batch_size\n",
    "    \n",
    "    pool = Pool(cpu_count())\n",
    "    res = pool.map(_get_train_batch, range(num_batch))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    user_list = [r[0] for r in res]\n",
    "    item_pos_list = [r[1] for r in res]\n",
    "    user_dns_list = [r[2] for r in res]\n",
    "    item_dns_list = [r[3] for r in res]\n",
    "    \n",
    "    return user_list, item_pos_list, user_dns_list, item_dns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_train_batch(i):\n",
    "    user_batch, item_batch = [], []\n",
    "    user_neg_batch, item_neg_batch = [], []\n",
    "    \n",
    "    begin = i * _batch_size\n",
    "    \n",
    "    for idx in range(begin, begin + _batch_size):\n",
    "        \n",
    "        user_batch.append(_user_input[_index[idx]])\n",
    "        item_batch.append(_item_input_pos[_index[idx]])\n",
    "        \n",
    "        for dns in range(1):\n",
    "            \n",
    "            user = _user_input[_index[idx]]\n",
    "            user_neg_batch.append(user)\n",
    "            \n",
    "            # negtive k\n",
    "            j = np.random.randint(_dataset.num_items)\n",
    "            \n",
    "            while j in _dataset.trainList[_user_input[_index[idx]]]:\n",
    "                j = np.random.randint(_dataset.num_items)\n",
    "            item_neg_batch.append(j)\n",
    "            \n",
    "    return np.array(user_batch)[:, None], np.array(item_batch)[:, None], \\\n",
    "           np.array(user_neg_batch)[:, None], np.array(item_neg_batch)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = shuffle(samples, 512, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import (Dataset, DataLoader, TensorDataset)\n",
    "train_dataset = TensorDataset(torch.tensor(train, dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for i in range(len(train_dataset[0][0])):\n",
    "#     tmp = []\n",
    "#     for j in range(4):\n",
    "#         tmp.append(train_dataset[j][0][i])\n",
    "#     data.append(tmp)\n",
    "\n",
    "# train_dataset = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_dataset\n",
    "* shape(4, samples)\n",
    "  * 0: user_id\n",
    "  * 1: item_pos\n",
    "  * 2: user_id\n",
    "  * 3: item_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPRのpytorch実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_eval_model():\n",
    "    pool = Pool(cpu_count())\n",
    "    feed_dicts = pool.map(evaluate_input, range(dataset.num_users))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return feed_dicts\n",
    "\n",
    "def evaluate_input(user):\n",
    "    # generate items_list\n",
    "    test_item = dataset.testRatings[user][1]\n",
    "    item_input = set(range(dataset.num_items)) - set(dataset.trainList[user])\n",
    "    if test_item in item_input:\n",
    "        item_input.remove(test_item)\n",
    "    item_input = list(item_input)\n",
    "    item_input.append(test_item)\n",
    "    user_input = np.full(len(item_input), user, dtype='int32')[:, None]\n",
    "    item_input = np.array(item_input)[:, None]\n",
    "    return user_input, item_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_d = init_eval_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルのクラスを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class APR(nn.Module):\n",
    "\n",
    "    def __init__(self, n_user, n_item,  dataset, adv=1, feed_dicts=None, k=8):        \n",
    "        super(APR, self).__init__()\n",
    "        self.reg = 1 \n",
    "        self.adv = adv\n",
    "        self.reg_adv = 1\n",
    "        self.eps = 0.5\n",
    "        \n",
    "        self.embedding_P = nn.Embedding(n_user, k)#.from_pretrained(emb_P)\n",
    "        self.embedding_Q = nn.Embedding(n_item, k)#.from_pretrained(emb_Q)\n",
    "        nn.init.normal_(self.embedding_P.weight, 0.0, 0.01)\n",
    "        nn.init.normal_(self.embedding_Q.weight, 0.0, 0.01)\n",
    "        \n",
    "        # adversarial\n",
    "        self.delta_P = nn.Embedding(n_user, k)\n",
    "        self.delta_Q = nn.Embedding(n_item, k)\n",
    "        \n",
    "        self.delta_P .weight.requires_grad = False\n",
    "        self.delta_Q .weight.requires_grad = False\n",
    "        \n",
    "        self.delta_P.weight = nn.Parameter(torch.zeros([n_user, k], dtype=torch.float32))\n",
    "        self.delta_Q.weight = nn.Parameter(torch.zeros([n_user, k], dtype=torch.float32))\n",
    "        \n",
    "        self.h = torch.ones(k, 1)\n",
    "        self.dataset = dataset\n",
    "        self.eval_data =  feed_dicts\n",
    "    \n",
    "    def forward(self, user, item):        \n",
    "        if self.adv == 0:\n",
    "            return torch.matmul(self.embedding_P(user.flatten()) * self.embedding_Q(item.flatten()), self.h).sum(1)\n",
    "        else:\n",
    "#             print(\"aaa\")\n",
    "            ##adversarialな計算\n",
    "            P_plus_delta = torch.add(self.embedding_P(user.flatten()), self.delta_P(user.flatten()))\n",
    "            Q_plus_delta = self.embedding_Q(user.flatten()) + self.delta_Q(user.flatten())\n",
    "            adv_term = torch.matmul(P_plus_delta * Q_plus_delta, self.h).sum(1)            \n",
    "            return torch.matmul(self.embedding_P(user.flatten()) * self.embedding_Q(item.flatten()), self.h).sum(1), adv_term \n",
    "                \n",
    "    def bpr_loss(self, user, items):\n",
    "        pos_preds = self.predict(user, items[0])\n",
    "        neg_preds = self.predict(user, items[1])\n",
    "        preds = pos_preds - neg_preds\n",
    "        loss = torch.log(1 + torch.exp(-preds)).sum()\n",
    "        return loss\n",
    "    \n",
    "    def apr_loss(self, user, items):\n",
    "        pos_preds_adv = self.predict_adv(user, items[0])\n",
    "        neg_preds_adv = self.predict_adv(user, items[1])\n",
    "        preds_adv = pos_preds_adv - neg_preds_adv \n",
    "        loss = self.reg_adv * torch.log(1 + torch.exp(-preds_adv)).sum()  \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, user, item):        \n",
    "        return torch.matmul(self.embedding_P(user.flatten()) * self.embedding_Q(item.flatten()), self.h).sum(1)\n",
    "    \n",
    "    def predict_adv(self, user, item):  \n",
    "        P_plus_delta = torch.add(self.embedding_P(user.flatten()), self.delta_P(user.flatten()))\n",
    "        Q_plus_delta = torch.add(self.embedding_Q(user.flatten()), self.delta_Q(user.flatten()))\n",
    "        return torch.matmul(P_plus_delta * Q_plus_delta, self.h).sum(1)\n",
    "                \n",
    "    def evaluate(self):\n",
    "        res = []\n",
    "        for user in range(self.dataset.num_users):\n",
    "            res.append(self.eval_by_user(user))\n",
    "        res = np.array(res)\n",
    "        hr, ndcg, auc = (res.mean(axis=0)).tolist()\n",
    "        return hr, ndcg, auc\n",
    "\n",
    "    def eval_by_user(self, user):\n",
    "\n",
    "        user_input, item_input = self.eval_data[user]\n",
    "\n",
    "        predictions = self.predict(torch.tensor(user_input, dtype=torch.long), torch.tensor(item_input, dtype=torch.long))\n",
    "        neg_predict, pos_predict = predictions[:-1], predictions[-1]\n",
    "        position = (neg_predict >= pos_predict).sum()\n",
    "        hr, ndcg, auc = [], [], []\n",
    "        for k in range(1,100 + 1):\n",
    "            hr.append(position < k)\n",
    "            ndcg.append(math.log(2) / math.log(position + 2) if position < k else 0)\n",
    "            auc.append(1 - (position / len(neg_predict)))\n",
    "\n",
    "        return hr, ndcg, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "adv = 1\n",
    "n_user = dataset.num_users\n",
    "n_item = dataset.num_items\n",
    "model = APR(n_user, n_item, dataset, adv, feed_dicts=eval_d)\n",
    "optimizer = optim.SGD(model.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "batch_num = len(train_dataset[0][0]) // 512 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch', 0)\n",
      "OrderedDict([('embedding_P.weight', tensor([[ 0.0136, -0.0127, -0.0041,  ..., -0.0166,  0.0077, -0.0093],\n",
      "        [ 0.0079,  0.0028, -0.0204,  ..., -0.0264,  0.0182, -0.0037],\n",
      "        [ 0.0162, -0.0018,  0.0103,  ..., -0.0082, -0.0049,  0.0065],\n",
      "        ...,\n",
      "        [ 0.0045,  0.0076, -0.0032,  ..., -0.0380,  0.0009, -0.0106],\n",
      "        [ 0.0089, -0.0063,  0.0002,  ...,  0.0042,  0.0111,  0.0008],\n",
      "        [-0.0034,  0.0050, -0.0103,  ...,  0.0071, -0.0031, -0.0184]])), ('embedding_Q.weight', tensor([[-0.0046,  0.0013,  0.0074,  ..., -0.0097,  0.0099,  0.0213],\n",
      "        [ 0.0005, -0.0055, -0.0036,  ..., -0.0084,  0.0148,  0.0167],\n",
      "        [-0.0006,  0.0021, -0.0013,  ..., -0.0186,  0.0252,  0.0108],\n",
      "        ...,\n",
      "        [-0.0056, -0.0200,  0.0218,  ..., -0.0055,  0.0076,  0.0064],\n",
      "        [-0.0014, -0.0123,  0.0103,  ..., -0.0041, -0.0014,  0.0177],\n",
      "        [-0.0032, -0.0024, -0.0064,  ...,  0.0003, -0.0017, -0.0046]])), ('delta_P.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('delta_Q.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]))])\n",
      "tensor(341.3928, grad_fn=<SumBackward0>)\n",
      "('epoch', 1)\n",
      "OrderedDict([('embedding_P.weight', tensor([[ 0.0036, -0.0347,  0.0033,  ..., -0.0244, -0.0098, -0.0311],\n",
      "        [ 0.0337, -0.0069, -0.0479,  ..., -0.0168,  0.0155, -0.0538],\n",
      "        [ 0.0210, -0.0033,  0.0060,  ..., -0.0017, -0.0063, -0.0007],\n",
      "        ...,\n",
      "        [ 0.0035,  0.0089, -0.0017,  ..., -0.0406,  0.0004, -0.0103],\n",
      "        [ 0.0063, -0.0079,  0.0009,  ...,  0.0044,  0.0089, -0.0015],\n",
      "        [-0.0034,  0.0028, -0.0097,  ...,  0.0085, -0.0031, -0.0230]])), ('embedding_Q.weight', tensor([[ 0.0019,  0.0094,  0.0055,  ..., -0.0123,  0.0210,  0.0330],\n",
      "        [ 0.0201, -0.0086, -0.0140,  ..., -0.0062,  0.0207,  0.0217],\n",
      "        [ 0.0046,  0.0025, -0.0047,  ..., -0.0270,  0.0386,  0.0227],\n",
      "        ...,\n",
      "        [-0.0071, -0.0187,  0.0195,  ..., -0.0055,  0.0098,  0.0140],\n",
      "        [ 0.0049, -0.0054,  0.0115,  ..., -0.0020,  0.0129,  0.0277],\n",
      "        [ 0.0023,  0.0028, -0.0086,  ...,  0.0016,  0.0074,  0.0022]])), ('delta_P.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('delta_Q.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]))])\n",
      "tensor(341.3928, grad_fn=<SumBackward0>)\n",
      "('epoch', 2)\n",
      "OrderedDict([('embedding_P.weight', tensor([[-0.0725, -0.1418,  0.0431,  ..., -0.0455, -0.1217, -0.1414],\n",
      "        [ 0.1657, -0.0989, -0.2148,  ...,  0.1133, -0.0491, -0.3556],\n",
      "        [ 0.0428, -0.0183, -0.0214,  ...,  0.0260, -0.0210, -0.0491],\n",
      "        ...,\n",
      "        [-0.0054,  0.0066,  0.0046,  ..., -0.0481, -0.0074, -0.0121],\n",
      "        [-0.0062, -0.0238,  0.0027,  ...,  0.0059, -0.0107, -0.0246],\n",
      "        [ 0.0012, -0.0032, -0.0157,  ...,  0.0163, -0.0061, -0.0396]])), ('embedding_Q.weight', tensor([[ 0.0119,  0.0368,  0.0109,  ..., -0.0255,  0.0575,  0.0804],\n",
      "        [ 0.0839, -0.0121, -0.0390,  ..., -0.0017,  0.0629,  0.0536],\n",
      "        [ 0.0311,  0.0153, -0.0111,  ..., -0.0443,  0.0900,  0.0736],\n",
      "        ...,\n",
      "        [-0.0097, -0.0072,  0.0279,  ..., -0.0151,  0.0287,  0.0523],\n",
      "        [ 0.0306,  0.0241,  0.0144,  ..., -0.0033,  0.0647,  0.0737],\n",
      "        [ 0.0118,  0.0239, -0.0066,  ..., -0.0015,  0.0360,  0.0361]])), ('delta_P.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('delta_Q.weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]))])\n",
      "tensor(341.3928, grad_fn=<SumBackward0>)\n",
      "eval\n",
      "(0.09818125170385948, 0.021816121680212838, 1.0)\n"
     ]
    }
   ],
   "source": [
    "from torch import autograd\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    print(\"epoch\", epoch)\n",
    "    for i in range(1312):\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        user = autograd.Variable(train_dataset[0][0][i])\n",
    "        item_pos = autograd.Variable(train_dataset[1][0][i])\n",
    "        item_neg = autograd.Variable(train_dataset[3][0][i])\n",
    "        \n",
    "        user = user.view(-1,)\n",
    "        item_pos = item_pos.view(-1,)\n",
    "        item_neg = item_neg.view(-1,)\n",
    "        \n",
    "\n",
    "        loss_bpr = model.bpr_loss(user, (item_pos, item_neg))\n",
    "        loss_bpr.backward()\n",
    "        \n",
    "        grad_P = model.embedding_P.weight.grad\n",
    "        grad_Q = model.embedding_Q.weight.grad\n",
    "        model.delta_P.weight =  nn.Parameter(model.eps * f.normalize(model.embedding_P.weight.grad, p=2, dim=1))\n",
    "        model.delta_Q.weight =  nn.Parameter(model.eps * f.normalize(model.embedding_Q.weight.grad, p=2, dim=1))\n",
    "        \n",
    "        loss_apr = model.apr_loss(user, (item_pos, item_neg))\n",
    "#         model.zero_grad()\n",
    "        loss_apr.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(model.state_dict())\n",
    "    print(loss)\n",
    "\n",
    "print(\"eval\")\n",
    "result = model.evaluate()\n",
    "hr, ndcg, auc = np.swapaxes(result, 0, 1)[-1]\n",
    "print(hr, ndcg, auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
